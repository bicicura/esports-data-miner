{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bicicura/esports-data-miner/blob/main/Spanish_Lora_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# ‚≠ê Entrenador de Lora de Hollowstrawberry\n",
        "\n",
        "Basado en el trabajo de [Kohya_ss](https://github.com/kohya-ss/sd-scripts) y [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb#scrollTo=-Z4w3lfFKLjr). ¬°Gracias!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### ‚≠ï Disclaimer\n",
        "The purpose of this document is to research bleeding-edge technologies in the field of machine learning.  \n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "U_oXgrthpAv-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|üá¨üáß English|üá™üá∏ Spanish|\n",
        "|:--|:-:|:-:|:-:|\n",
        "| üè† **Origen** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | | |\n",
        "| üìä **Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n",
        "| ‚≠ê **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |\n",
        "| üåü **XL Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) |  |\n",
        "| üåü **Legacy XL Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) |  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OglZzI_ujZq-",
        "outputId": "be93f57b-f0a6-46df-c889-7c307c92b2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíø Revisando archivos...\n",
            "üìÅMyDrive/Loras/bicicura/dataset\n",
            "üìà Se encontraron 49 im√°genes con 6 repeticiones, equivalente a 294 pasos.\n",
            "üìâ Divide 294 pasos en 1 batch size para obtener 294.0 pasos por epoch.\n",
            "üîÆ Habr√° 7 epochs, para un total de alrededor de 2058 pasos totales.\n",
            "\n",
            "üè≠ Instalando...\n",
            "\n",
            "Cloning into '/content/kohya-trainer'...\n",
            "remote: Enumerating objects: 10785, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 10785 (delta 56), reused 41 (delta 33), pack-reused 10681 (from 2)\u001b[K\n",
            "Receiving objects: 100% (10785/10785), 12.66 MiB | 6.25 MiB/s, done.\n",
            "Resolving deltas: 100% (7707/7707), done.\n",
            "95 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "aria2 is already the newest version (1.36.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 95 not upgraded.\n",
            "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Patched library/train_util.py OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Instalaci√≥n completada en 32 segundos.\n",
            "\n",
            "üîÑ Descargando modelo...\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "c1b96b|\u001b[1;32mOK\u001b[0m  |       0B/s|//content/sd-v1-5-pruned-noema-fp16.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "\n",
            "üìÑ Configuraci√≥n guardada en /content/drive/MyDrive/Loras/bicicura/training_config.toml\n",
            "üìÑ Configuraci√≥n de datos guardada en /content/drive/MyDrive/Loras/bicicura/dataset_config.toml\n",
            "\n",
            "‚≠ê Iniciando entrenador...\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.12/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768785227.238527   21410 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768785227.244644   21410 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768785227.260159   21410 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768785227.260189   21410 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768785227.260193   21410 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768785227.260197   21410 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/usr/local/lib/python3.12/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/content/kohya-trainer/library/custom_train_functions.py:172: SyntaxWarning: invalid escape sequence '\\('\n",
            "  \\( - literal character '('\n",
            "/content/kohya-trainer/library/lpw_stable_diffusion.py:70: SyntaxWarning: invalid escape sequence '\\('\n",
            "  \\( - literal character '('\n",
            "\u001b[2;36m2026-01-19 01:13:52\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading settings from            \u001b]8;id=68485;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=527632;file:///content/kohya-trainer/library/train_util.py#4016\u001b\\\u001b[2m4016\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/\u001b[0m\u001b[95mtraining_config.toml...\u001b[0m    \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b]8;id=53882;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=570671;file:///content/kohya-trainer/library/train_util.py#4035\u001b\\\u001b[2m4035\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/\u001b[0m\u001b[95mtraining_config\u001b[0m            \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m2026-01-19 01:13:52\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m prepare tokenizer                \u001b]8;id=256787;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=234053;file:///content/kohya-trainer/library/train_util.py#4562\u001b\\\u001b[2m4562\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m update token length: \u001b[1;36m225\u001b[0m         \u001b]8;id=935518;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=571858;file:///content/kohya-trainer/library/train_util.py#4579\u001b\\\u001b[2m4579\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading dataset config from    \u001b]8;id=229258;file:///content/kohya-trainer/train_network.py\u001b\\\u001b[2mtrain_network.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=243962;file:///content/kohya-trainer/train_network.py#161\u001b\\\u001b[2m161\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35m/content/drive/MyDrive/Loras/b\u001b[0m \u001b[2m                    \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micicura/\u001b[0m\u001b[95mdataset_config.toml\u001b[0m    \u001b[2m                    \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m prepare images.                  \u001b]8;id=750800;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=681453;file:///content/kohya-trainer/library/train_util.py#1690\u001b\\\u001b[2m1690\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m found directory                  \u001b]8;id=617889;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=291704;file:///content/kohya-trainer/library/train_util.py#1637\u001b\\\u001b[2m1637\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/\u001b[0m\u001b[95mdataset\u001b[0m contains \u001b[1;36m49\u001b[0m image  \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         files                            \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m No caption file found for \u001b[1;36m49\u001b[0m     \u001b]8;id=167414;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=732052;file:///content/kohya-trainer/library/train_util.py#1668\u001b\\\u001b[2m1668\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         images. Training will continue   \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         without captions for these       \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         images. If class token exists,   \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         it will be used. \u001b[35m/\u001b[0m               \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         49Êûö„ÅÆÁîªÂÉè„Å´„Ç≠„É£„Éó„Ç∑„Éß„É≥„Éï„Ç°„Ç§„É´ \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ„Åì„Çå„Çâ„ÅÆ \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         ÁîªÂÉè„Å´„Å§„ÅÑ„Å¶„ÅØ„Ç≠„É£„Éó„Ç∑„Éß„É≥„Å™„Åó„Åß \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         Â≠¶Áøí„ÇíÁ∂öË°å„Åó„Åæ„Åô„ÄÇclass          \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         token„ÅåÂ≠òÂú®„Åô„ÇãÂ†¥Âêà„ÅØ„Åù„Çå„Çí‰Ωø„ÅÑ  \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         „Åæ„Åô„ÄÇ                           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b]8;id=800581;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=352944;file:///content/kohya-trainer/library/train_util.py#1675\u001b\\\u001b[2m1675\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/dataset/\u001b[0m\u001b[95m056FDD18-DA9A-48F3\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[95m-9B32-A3143D76CCD8.JPG\u001b[0m           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b]8;id=376417;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=888662;file:///content/kohya-trainer/library/train_util.py#1675\u001b\\\u001b[2m1675\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/dataset/\u001b[0m\u001b[95m0A6B731D-67BA-42A2\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[95m-9A90-35CA5A873C69.JPG\u001b[0m           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b]8;id=45561;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=765179;file:///content/kohya-trainer/library/train_util.py#1675\u001b\\\u001b[2m1675\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/dataset/\u001b[0m\u001b[95m0F70AF14-DEC0-4280\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[95m-B357-ABEFE6F47C7B.JPG\u001b[0m           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b]8;id=396922;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=82627;file:///content/kohya-trainer/library/train_util.py#1675\u001b\\\u001b[2m1675\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/dataset/\u001b[0m\u001b[95m18476241-C52E-4E06\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[95m-98A3-588DE3E319BF.JPG\u001b[0m           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b]8;id=648564;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=928463;file:///content/kohya-trainer/library/train_util.py#1675\u001b\\\u001b[2m1675\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/dataset/\u001b[0m\u001b[95m5ACD6C93-AC77-4D95\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[95m-A096-D302187AE5E0.JPG\u001b[0m           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m \u001b[35m/content/drive/MyDrive/Loras/bic\u001b[0m \u001b]8;id=72933;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=48050;file:///content/kohya-trainer/library/train_util.py#1673\u001b\\\u001b[2m1673\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35micura/dataset/\u001b[0m\u001b[95m68CCF355-A275-4130\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[95m-9AC1-FF45BAF30F08.JPG...\u001b[0m and \u001b[1;36m44\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         more                             \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m294\u001b[0m train images with repeating. \u001b]8;id=896865;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=244098;file:///content/kohya-trainer/library/train_util.py#1731\u001b\\\u001b[2m1731\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m0\u001b[0m reg images.                    \u001b]8;id=666563;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=874628;file:///content/kohya-trainer/library/train_util.py#1734\u001b\\\u001b[2m1734\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m no regularization images \u001b[35m/\u001b[0m       \u001b]8;id=702729;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=279946;file:///content/kohya-trainer/library/train_util.py#1739\u001b\\\u001b[2m1739\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         Ê≠£ÂâáÂåñÁîªÂÉè„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mDataset \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m                      \u001b]8;id=665822;file:///content/kohya-trainer/library/config_util.py\u001b\\\u001b[2mconfig_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=179451;file:///content/kohya-trainer/library/config_util.py#572\u001b\\\u001b[2m572\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m           batch_size: \u001b[1;36m1\u001b[0m                  \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m           resolution: \u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m)\u001b[0m         \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m           enable_bucket: \u001b[3;92mTrue\u001b[0m            \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m           network_multiplier: \u001b[1;36m1.0\u001b[0m        \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m           min_bucket_reso: \u001b[1;36m256\u001b[0m           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m           max_bucket_reso: \u001b[1;36m1024\u001b[0m          \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m           bucket_reso_steps: \u001b[1;36m64\u001b[0m          \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m           bucket_no_upscale: \u001b[3;91mFalse\u001b[0m       \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m                                          \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m           \u001b[1m[\u001b[0mSubset \u001b[1;36m0\u001b[0m of Dataset \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m        \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             image_dir:                   \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[32m\"/content/drive/MyDrive/Loras/bi\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[32mcicura/dataset\"\u001b[0m                  \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             image_count: \u001b[1;36m49\u001b[0m              \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             num_repeats: \u001b[1;36m6\u001b[0m               \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             shuffle_caption: \u001b[3;91mFalse\u001b[0m       \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             keep_tokens: \u001b[1;36m0\u001b[0m               \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             keep_tokens_separator:       \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             caption_separator: ,         \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             secondary_separator: \u001b[3;35mNone\u001b[0m    \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             enable_wildcard: \u001b[3;91mFalse\u001b[0m       \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             caption_dropout_rate: \u001b[1;36m0.0\u001b[0m    \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             caption_dropout_every_n_epoc \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         hes: \u001b[1;36m0\u001b[0m                           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             caption_tag_dropout_rate:    \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0.0\u001b[0m                              \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             caption_prefix: \u001b[3;35mNone\u001b[0m         \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             caption_suffix: \u001b[3;35mNone\u001b[0m         \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             color_aug: \u001b[3;91mFalse\u001b[0m             \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             flip_aug: \u001b[3;91mFalse\u001b[0m              \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             face_crop_aug_range: \u001b[3;35mNone\u001b[0m    \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             random_crop: \u001b[3;91mFalse\u001b[0m           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             token_warmup_min: \u001b[1;36m1\u001b[0m,         \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             token_warmup_step: \u001b[1;36m0\u001b[0m,        \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             alpha_mask: \u001b[3;91mFalse\u001b[0m,           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             is_reg: \u001b[3;91mFalse\u001b[0m                \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             class_tokens: bicicura       \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m             caption_extension:           \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m                                          \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m                                          \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mDataset \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m                      \u001b]8;id=671088;file:///content/kohya-trainer/library/config_util.py\u001b\\\u001b[2mconfig_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=721590;file:///content/kohya-trainer/library/config_util.py#578\u001b\\\u001b[2m578\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading image sizes.              \u001b]8;id=805635;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=813694;file:///content/kohya-trainer/library/train_util.py#902\u001b\\\u001b[2m902\u001b[0m\u001b]8;;\u001b\\\n",
            "100% 49/49 [00:00<00:00, 452.77it/s]\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m make buckets                      \u001b]8;id=330776;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=420651;file:///content/kohya-trainer/library/train_util.py#908\u001b\\\u001b[2m908\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m number of images \u001b[1m(\u001b[0mincluding       \u001b]8;id=594731;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=918938;file:///content/kohya-trainer/library/train_util.py#954\u001b\\\u001b[2m954\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         repeats\u001b[1m)\u001b[0m \u001b[35m/\u001b[0m                        \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         ÂêÑbucket„ÅÆÁîªÂÉèÊûöÊï∞ÔºàÁπ∞„ÇäËøî„ÅóÂõûÊï∞  \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         „ÇíÂê´„ÇÄÔºâ                          \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m bucket \u001b[1;36m0\u001b[0m: resolution \u001b[1m(\u001b[0m\u001b[1;36m320\u001b[0m, \u001b[1;36m704\u001b[0m\u001b[1m)\u001b[0m,  \u001b]8;id=414850;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=927657;file:///content/kohya-trainer/library/train_util.py#959\u001b\\\u001b[2m959\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         count: \u001b[1;36m42\u001b[0m                         \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m bucket \u001b[1;36m1\u001b[0m: resolution \u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m, \u001b[1;36m640\u001b[0m\u001b[1m)\u001b[0m,  \u001b]8;id=277746;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146413;file:///content/kohya-trainer/library/train_util.py#959\u001b\\\u001b[2m959\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         count: \u001b[1;36m90\u001b[0m                         \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m bucket \u001b[1;36m2\u001b[0m: resolution \u001b[1m(\u001b[0m\u001b[1;36m448\u001b[0m, \u001b[1;36m576\u001b[0m\u001b[1m)\u001b[0m,  \u001b]8;id=275504;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=783300;file:///content/kohya-trainer/library/train_util.py#959\u001b\\\u001b[2m959\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         count: \u001b[1;36m96\u001b[0m                         \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m bucket \u001b[1;36m3\u001b[0m: resolution \u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m)\u001b[0m,  \u001b]8;id=418801;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=379580;file:///content/kohya-trainer/library/train_util.py#959\u001b\\\u001b[2m959\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         count: \u001b[1;36m24\u001b[0m                         \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m bucket \u001b[1;36m4\u001b[0m: resolution \u001b[1m(\u001b[0m\u001b[1;36m576\u001b[0m, \u001b[1;36m448\u001b[0m\u001b[1m)\u001b[0m,  \u001b]8;id=95325;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=792495;file:///content/kohya-trainer/library/train_util.py#959\u001b\\\u001b[2m959\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         count: \u001b[1;36m24\u001b[0m                         \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m bucket \u001b[1;36m5\u001b[0m: resolution \u001b[1m(\u001b[0m\u001b[1;36m640\u001b[0m, \u001b[1;36m384\u001b[0m\u001b[1m)\u001b[0m,  \u001b]8;id=657924;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=167753;file:///content/kohya-trainer/library/train_util.py#959\u001b\\\u001b[2m959\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         count: \u001b[1;36m18\u001b[0m                         \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m mean ar error \u001b[1m(\u001b[0mwithout repeats\u001b[1m)\u001b[0m:  \u001b]8;id=403457;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=400156;file:///content/kohya-trainer/library/train_util.py#967\u001b\\\u001b[2m967\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0.03602939585384106\u001b[0m               \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m preparing accelerator          \u001b]8;id=714087;file:///content/kohya-trainer/train_network.py\u001b\\\u001b[2mtrain_network.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=849609;file:///content/kohya-trainer/train_network.py#225\u001b\\\u001b[2m225\u001b[0m\u001b]8;;\u001b\\\n",
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "accelerator device: cuda\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading model for process \u001b[1;36m0\u001b[0m/\u001b[1;36m1\u001b[0m    \u001b]8;id=40039;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=820106;file:///content/kohya-trainer/library/train_util.py#4720\u001b\\\u001b[2m4720\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m load StableDiffusion checkpoint: \u001b]8;id=477595;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=373873;file:///content/kohya-trainer/library/train_util.py#4676\u001b\\\u001b[2m4676\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[35m/content/\u001b[0m\u001b[95msd-v1-5-pruned-noema-fp\u001b[0m \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[95m16.safetensors\u001b[0m                   \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m UNet2DConditionModel: \u001b[1;36m64\u001b[0m, \u001b[1;36m8\u001b[0m,  \u001b]8;id=41922;file:///content/kohya-trainer/library/original_unet.py\u001b\\\u001b[2moriginal_unet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=509781;file:///content/kohya-trainer/library/original_unet.py#1387\u001b\\\u001b[2m1387\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[1;36m768\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m             \u001b[2m                     \u001b[0m\n",
            "\u001b[2;36m2026-01-19 01:14:04\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading u-net: \u001b[1m<\u001b[0m\u001b[1;95mAll\u001b[0m\u001b[39m keys matched\u001b[0m \u001b]8;id=596908;file:///content/kohya-trainer/library/model_util.py\u001b\\\u001b[2mmodel_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=599572;file:///content/kohya-trainer/library/model_util.py#1008\u001b\\\u001b[2m1008\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[39msuccessfully\u001b[0m\u001b[1m>\u001b[0m                    \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m2026-01-19 01:14:05\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading vae: \u001b[1m<\u001b[0m\u001b[1;95mAll\u001b[0m\u001b[39m keys matched \u001b[0m  \u001b]8;id=859197;file:///content/kohya-trainer/library/model_util.py\u001b\\\u001b[2mmodel_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=927660;file:///content/kohya-trainer/library/model_util.py#1016\u001b\\\u001b[2m1016\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[39msuccessfully\u001b[0m\u001b[1m>\u001b[0m                    \u001b[2m                  \u001b[0m\n",
            "\u001b[2;36m2026-01-19 01:14:07\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading text encoder: \u001b[1m<\u001b[0m\u001b[1;95mAll\u001b[0m\u001b[39m keys \u001b[0m \u001b]8;id=549151;file:///content/kohya-trainer/library/model_util.py\u001b\\\u001b[2mmodel_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=440518;file:///content/kohya-trainer/library/model_util.py#1073\u001b\\\u001b[2m1073\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[39mmatched successfully\u001b[0m\u001b[1m>\u001b[0m            \u001b[2m                  \u001b[0m\n",
            "import network module: networks.lora\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mDataset \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m                      \u001b]8;id=446662;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=935164;file:///content/kohya-trainer/library/train_util.py#2199\u001b\\\u001b[2m2199\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m caching latents.                 \u001b]8;id=109853;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=435831;file:///content/kohya-trainer/library/train_util.py#1026\u001b\\\u001b[2m1026\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m checking cache validity\u001b[33m...\u001b[0m       \u001b]8;id=769947;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=32192;file:///content/kohya-trainer/library/train_util.py#1053\u001b\\\u001b[2m1053\u001b[0m\u001b]8;;\u001b\\\n",
            "100% 49/49 [00:00<00:00, 536608.08it/s]\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m caching latents\u001b[33m...\u001b[0m               \u001b]8;id=31598;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=520748;file:///content/kohya-trainer/library/train_util.py#1095\u001b\\\u001b[2m1095\u001b[0m\u001b]8;;\u001b\\\n",
            "100% 49/49 [00:24<00:00,  2.01it/s]\n",
            "\u001b[2;36m2026-01-19 01:14:32\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m create LoRA network. base dim \u001b[1m(\u001b[0mrank\u001b[1m)\u001b[0m:   \u001b]8;id=369626;file:///content/kohya-trainer/networks/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=73745;file:///content/kohya-trainer/networks/lora.py#935\u001b\\\u001b[2m935\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[1;36m16\u001b[0m, alpha: \u001b[1;36m8\u001b[0m                            \u001b[2m           \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m neuron dropout: \u001b[33mp\u001b[0m=\u001b[3;35mNone\u001b[0m, rank dropout:   \u001b]8;id=364776;file:///content/kohya-trainer/networks/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=186549;file:///content/kohya-trainer/networks/lora.py#936\u001b\\\u001b[2m936\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[33mp\u001b[0m=\u001b[3;35mNone\u001b[0m, module dropout: \u001b[33mp\u001b[0m=\u001b[3;35mNone\u001b[0m          \u001b[2m           \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m create LoRA for Text Encoder:          \u001b]8;id=73943;file:///content/kohya-trainer/networks/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=625568;file:///content/kohya-trainer/networks/lora.py#1030\u001b\\\u001b[2m1030\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m create LoRA for Text Encoder: \u001b[1;36m72\u001b[0m       \u001b]8;id=691030;file:///content/kohya-trainer/networks/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=706487;file:///content/kohya-trainer/networks/lora.py#1035\u001b\\\u001b[2m1035\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         modules.                               \u001b[2m            \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m create LoRA for U-Net: \u001b[1;36m192\u001b[0m modules.    \u001b]8;id=7533;file:///content/kohya-trainer/networks/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=307508;file:///content/kohya-trainer/networks/lora.py#1043\u001b\\\u001b[2m1043\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m enable LoRA for text encoder: \u001b[1;36m72\u001b[0m       \u001b]8;id=244174;file:///content/kohya-trainer/networks/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=900910;file:///content/kohya-trainer/networks/lora.py#1084\u001b\\\u001b[2m1084\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         modules                                \u001b[2m            \u001b[0m\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m enable LoRA for U-Net: \u001b[1;36m192\u001b[0m modules     \u001b]8;id=361181;file:///content/kohya-trainer/networks/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=741847;file:///content/kohya-trainer/networks/lora.py#1089\u001b\\\u001b[2m1089\u001b[0m\u001b]8;;\u001b\\\n",
            "prepare optimizer, data loader etc.\n",
            "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m use AdamW optimizer | \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m         \u001b]8;id=351286;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=496364;file:///content/kohya-trainer/library/train_util.py#4369\u001b\\\u001b[2m4369\u001b[0m\u001b]8;;\u001b\\\n",
            "override steps. steps for 7 epochs is / ÊåáÂÆö„Ç®„Éù„ÉÉ„ÇØ„Åæ„Åß„ÅÆ„Çπ„ÉÜ„ÉÉ„ÉóÊï∞: 2058\n",
            "running training / Â≠¶ÁøíÈñãÂßã\n",
            "  num train images * repeats / Â≠¶ÁøíÁîªÂÉè„ÅÆÊï∞√óÁπ∞„ÇäËøî„ÅóÂõûÊï∞: 294\n",
            "  num reg images / Ê≠£ÂâáÂåñÁîªÂÉè„ÅÆÊï∞: 0\n",
            "  num batches per epoch / 1epoch„ÅÆ„Éê„ÉÉ„ÉÅÊï∞: 294\n",
            "  num epochs / epochÊï∞: 7\n",
            "  batch size per device / „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫: 1\n",
            "  gradient accumulation steps / ÂãæÈÖç„ÇíÂêàË®à„Åô„Çã„Çπ„ÉÜ„ÉÉ„ÉóÊï∞ = 1\n",
            "  total optimization steps / Â≠¶Áøí„Çπ„ÉÜ„ÉÉ„ÉóÊï∞: 2058\n",
            "steps:   0% 0/2058 [00:00<?, ?it/s]\n",
            "epoch 1/7\n",
            "\u001b[2;36m2026-01-19 01:14:40\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m epoch is incremented.             \u001b]8;id=424725;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=156565;file:///content/kohya-trainer/library/train_util.py#694\u001b\\\u001b[2m694\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         current_epoch: \u001b[1;36m0\u001b[0m, epoch: \u001b[1;36m1\u001b[0m        \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m2026-01-19 01:14:40\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m epoch is incremented.             \u001b]8;id=900460;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=133412;file:///content/kohya-trainer/library/train_util.py#694\u001b\\\u001b[2m694\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         current_epoch: \u001b[1;36m0\u001b[0m, epoch: \u001b[1;36m1\u001b[0m        \u001b[2m                 \u001b[0m\n",
            "steps:  14% 294/2058 [02:20<14:04,  2.09it/s, avr_loss=0.105]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/bicicura/output/bicicura-01.safetensors\n",
            "\n",
            "epoch 2/7\n",
            "\u001b[2;36m2026-01-19 01:17:00\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m epoch is incremented.             \u001b]8;id=985251;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=18964;file:///content/kohya-trainer/library/train_util.py#694\u001b\\\u001b[2m694\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         current_epoch: \u001b[1;36m1\u001b[0m, epoch: \u001b[1;36m2\u001b[0m        \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m2026-01-19 01:17:00\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m epoch is incremented.             \u001b]8;id=985251;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=18964;file:///content/kohya-trainer/library/train_util.py#694\u001b\\\u001b[2m694\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         current_epoch: \u001b[1;36m1\u001b[0m, epoch: \u001b[1;36m2\u001b[0m        \u001b[2m                 \u001b[0m\n",
            "steps:  29% 588/2058 [04:42<11:45,  2.08it/s, avr_loss=0.0984]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/bicicura/output/bicicura-02.safetensors\n",
            "\n",
            "epoch 3/7\n",
            "\u001b[2;36m2026-01-19 01:19:22\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m epoch is incremented.             \u001b]8;id=76922;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=608208;file:///content/kohya-trainer/library/train_util.py#694\u001b\\\u001b[2m694\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         current_epoch: \u001b[1;36m2\u001b[0m, epoch: \u001b[1;36m3\u001b[0m        \u001b[2m                 \u001b[0m\n",
            "\u001b[2;36m2026-01-19 01:19:22\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m epoch is incremented.             \u001b]8;id=76922;file:///content/kohya-trainer/library/train_util.py\u001b\\\u001b[2mtrain_util.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=608208;file:///content/kohya-trainer/library/train_util.py#694\u001b\\\u001b[2m694\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         current_epoch: \u001b[1;36m2\u001b[0m, epoch: \u001b[1;36m3\u001b[0m        \u001b[2m                 \u001b[0m\n",
            "steps:  37% 763/2058 [06:07<10:23,  2.08it/s, avr_loss=0.102]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "import shutil\n",
        "import zipfile\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "if \"optimizer\" not in globals():\n",
        "  optimizer = \"AdamW\"\n",
        "if \"optimizer_args\" not in globals():\n",
        "  optimizer_args = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "if \"weighted_captions\" not in globals():\n",
        "  weighted_captions = False\n",
        "if \"adjust_tags\" not in globals():\n",
        "  adjust_tags = False\n",
        "if \"keep_tokens_weight\" not in globals():\n",
        "  keep_tokens_weight = 1.0\n",
        "\n",
        "COLAB = True # low ram\n",
        "XFORMERS = False\n",
        "SOURCE = \"https://github.com/kohya-ss/sd-scripts\"\n",
        "COMMIT = None\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "#@title ## üö© Empieza Aqu√≠\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Base\n",
        "#@markdown El nombre de tu proyecto tambi√©n es el nombre de la carpeta donde ir√°n tus im√°genes. No se permiten espacios.\n",
        "nombre_proyecto = \"bicicura\" #@param {type:\"string\"}\n",
        "project_name = nombre_proyecto.strip()\n",
        "#@markdown La estructura de carpetas no importa y es por comodidad. Aseg√∫rate de siempre elegir la misma. Me gusta organizar por proyecto.\n",
        "estructura_de_carpetas = \"Organizar por proyecto (MyDrive/Loras/nombre_proyecto/dataset)\" #@param [\"Organizar por categor√≠a (MyDrive/lora_training/datasets/nombre_proyecto)\", \"Organizar por proyecto (MyDrive/Loras/nombre_proyecto/dataset)\"]\n",
        "folder_structure = estructura_de_carpetas\n",
        "#@markdown Decidir el modelo base de entrenamiento. Los modelos por defecto producen los resultados m√°s limpios y consistentes. Puedes cambiarlo por un modelo propio si lo deseas.\n",
        "modelo_de_entrenamiento = \"Stable Diffusion (sd-v1-5-pruned-noema-fp16.safetensors)\" #@param [\"Anime (animefull-final-pruned-fp16.safetensors)\", \"AnyLora (AnyLoRA_noVae_fp16-pruned.ckpt)\", \"Stable Diffusion (sd-v1-5-pruned-noema-fp16.safetensors)\"]\n",
        "opcional_enlace_a_modelo_propio = \"\" #@param {type:\"string\"}\n",
        "modelo_propio_basado_en_sd2 = False #@param {type:\"boolean\"}\n",
        "custom_model_is_based_on_sd2 = modelo_propio_basado_en_sd2\n",
        "\n",
        "if opcional_enlace_a_modelo_propio:\n",
        "  model_url = opcional_enlace_a_modelo_propio\n",
        "elif \"AnyLora\" in modelo_de_entrenamiento:\n",
        "  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.ckpt\"\n",
        "elif \"Anime\" in modelo_de_entrenamiento:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "else:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Procesamiento <p>\n",
        "#@markdown La resoluci√≥n de 512 es est√°ndar en Stable Diffusion 1.5. No es necesario recortar o achicar, el proceso es autom√°tico.\n",
        "resolucion = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "resolution = resolucion\n",
        "#@markdown Esta opci√≥n va a voltear tus im√°genes para as√≠ tener el doble y aprender mejor. <p>\n",
        "#@markdown **Desactiva esto si te importan los elementos asim√©tricos en tu Lora.**\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "#markdown Leave empty for no captions.\n",
        "caption_extension = \"\" #param {type:\"string\"}\n",
        "#@markdown Mezclar las tags de anime ayuda al aprendizaje. Una tag de activaci√≥n va al inicio de cada archivo de texto y no se mezclar√°.\n",
        "mezclar_tags = False #@param {type:\"boolean\"}\n",
        "shuffle_caption = mezclar_tags\n",
        "tags_de_activacion = \"0\" #@param [0,1,2,3]\n",
        "keep_tokens = int(tags_de_activacion)\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Pasos <p>\n",
        "#@markdown Tus im√°genes se repetir√°n este n√∫mero de veces durante el entrenamiento. Recomiendo que el valor total sea entre 200 y 400.\n",
        "num_repeats = 6 #@param {type:\"number\"}\n",
        "#@markdown Cu√°nto tiempo deseas entrenar. Un buen punto de partida puede ser alrededor de 10 epochs o alrededor de 2000 pasos. <p>\n",
        "#@markdown Un epoch es una cantidad de pasos igual a: tu cantidad de im√°genes multipliccada por sus repeticiones, y dividido en el batch size.\n",
        "unidad_preferida = \"Epochs\" #@param [\"Epochs\", \"Pasos\"]\n",
        "cuantos = 7 #@param {type:\"number\"}\n",
        "max_train_epochs = cuantos if unidad_preferida == \"Epochs\" else None\n",
        "max_train_steps = cuantos if unidad_preferida == \"Pasos\" else None\n",
        "#@markdown Guardar m√°s epochs te permitir√° comparar mejor el progreso de tu Lora.\n",
        "guardar_cada_cuantos_epochs = 1 #@param {type:\"number\"}\n",
        "save_every_n_epochs = guardar_cada_cuantos_epochs\n",
        "guardar_solo_ultimos_epochs = 10 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = guardar_solo_ultimos_epochs\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "#@markdown Un batch size mayor hace el entrenamiento m√°s r√°pido, pero puede empeorar el aprendizaje. Se recomienda 2 o 3.\n",
        "batch_size = 1 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "train_batch_size = batch_size\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Aprendizaje\n",
        "#@markdown La tasa de aprendizaje es lo m√°s importante. Si deseas entrenar m√°s lento con muchas im√°genes, o si tienes un dim y alpha altos, usa un unet de 2e-4 o menor. <p>\n",
        "#@markdown El text encoder ayuda a tu Lora a aprender conceptos un poco mejor. Se recomienda la mitad o un quinto del unet. Puedes dejarlo en 0 para algunos estilos.\n",
        "aprendizaje_unet = 2e-4 #@param {type:\"number\"}\n",
        "unet_lr = aprendizaje_unet\n",
        "aprendizaje_text_encoder = 1e-5 #@param {type:\"number\"}\n",
        "text_encoder_lr = aprendizaje_text_encoder\n",
        "#@markdown El scheduler es el algoritmo matem√°tico que guiar√° el entrenamiento. Para personajes recomiendo `cosine_with_restarts` con un valor de 3. Si no est√°s seguro ponlo en `constant` e ignora el valor.\n",
        "scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler = scheduler\n",
        "valor_de_scheduler = 3 #@param {type:\"number\"}\n",
        "lr_scheduler_number = valor_de_scheduler\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "#@markdown Pasos de calentamiento durante el entrenamiento para un inicio eficiente. Recomiendo dejarlo en 5%.\n",
        "calentamiento = 0.05 #@param {type:\"slider\", min:0.0, max:0.5, step:0.01}\n",
        "lr_warmup_ratio = calentamiento\n",
        "lr_warmup_steps = 0\n",
        "#@markdown Nueva funci√≥n que hace el aprendizaje mucho m√°s eficiente. Puede que tus Loras est√©n listos en la mitad de epochs. Se usar√° un valor de 5.0 como en la [investigaci√≥n](https://arxiv.org/abs/2303.09556).\n",
        "min_snr_gamma = True #@param {type:\"boolean\"}\n",
        "min_snr_gamma_value = 5.0 if min_snr_gamma else None\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Estructura\n",
        "#@markdown LoRA es el cl√°sico y √∫til para muchos usos. LoCon es bueno con estilos ya que aprende con m√°s capas.\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon\"]\n",
        "\n",
        "#@markdown Aqu√≠ hay algunos valores recomendados para las opciones de abajo:\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | LoRA | 16 | 8 |   |   |\n",
        "#@markdown | LoCon | 16 | 8 | 8 | 4 |\n",
        "\n",
        "#@markdown Un dim mayor equivale a un Lora m√°s grande, pero no siempre es mejor. Se recomienda de 8 a 32, con un alpha igual a la mitad del dim.\n",
        "network_dim = 16 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "network_alpha = 8 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "#@markdown Los siguientes s√≥lo aplican a las capas adicionales de LoCon.\n",
        "conv_dim = 8 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_alpha = 4 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Listo\n",
        "#@markdown Ahora puedes correr esta celda apretando el bot√≥n circular a la izquierda. ¬°Buena suerte!\n",
        "\n",
        "\n",
        "# üë©‚Äçüíª Cool code goes here\n",
        "\n",
        "if optimizer.lower() == \"prodigy\" or \"dadapt\" in optimizer.lower():\n",
        "  if override_values_for_dadapt_and_prodigy:\n",
        "    unet_lr = 0.5\n",
        "    text_encoder_lr = 0.5\n",
        "    lr_scheduler = \"constant_with_warmup\"\n",
        "    lr_warmup_ratio = 0.05\n",
        "    network_alpha = network_dim\n",
        "\n",
        "  if not optimizer_args:\n",
        "    optimizer_args = [\"decouple=True\",\"weight_decay=0.01\",\"betas=[0.9,0.999]\"]\n",
        "    if optimizer == \"Prodigy\":\n",
        "      optimizer_args.extend([\"d_coef=2\",\"use_bias_correction=True\"])\n",
        "      if lr_warmup_ratio > 0:\n",
        "        optimizer_args.append(\"safeguard_warmup=True\")\n",
        "      else:\n",
        "        optimizer_args.append(\"safeguard_warmup=False\")\n",
        "\n",
        "root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  os.chdir(root_dir)\n",
        "\n",
        "  # ‚úÖ Re-clonar limpio (evita l√≠os de git pull + sed patches)\n",
        "  if os.path.exists(repo_dir):\n",
        "    !rm -rf \"{repo_dir}\"\n",
        "  !git clone {SOURCE} \"{repo_dir}\"\n",
        "\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  if COMMIT:\n",
        "    !git reset --hard \"{COMMIT}\"\n",
        "\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/train_network_wrapper.py -q -O train_network_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "\n",
        "  # ‚úÖ Fix numpy/opencv (lo m√°s importante para cv2)\n",
        "  !pip -q install \"numpy<2\"\n",
        "  !pip -q uninstall -y opencv-python opencv-contrib-python opencv-python-headless\n",
        "  !pip -q install \"opencv-python-headless==4.8.1.78\"\n",
        "\n",
        "  # ‚úÖ Dependencias del trainer\n",
        "  !pip -q install accelerate==0.25.0 transformers==4.36.2 diffusers[torch]==0.25.0 ftfy==6.1.1 \\\n",
        "    einops==0.7.0 pytorch-lightning==1.9.0 \\\n",
        "    bitsandbytes==0.43.0 \\\n",
        "    prodigyopt==1.0 lion-pytorch==0.0.6 tensorboard safetensors==0.4.2 altair==4.2.2 \\\n",
        "    easygui==0.98.3 toml==0.10.2 voluptuous==0.13.1 huggingface-hub==0.20.1 imagesize==1.4.1 \\\n",
        "    rich==13.7.1 torch==2.4.1 triton\n",
        "\n",
        "  !pip -q install -e .\n",
        "\n",
        "  if XFORMERS:\n",
        "    !pip -q install xformers==0.0.28.post1\n",
        "\n",
        "  # patches (robusto a distintos repos)\n",
        "  if COLAB:\n",
        "    # algunos forks ten√≠an este archivo; si no existe, no pasa nada\n",
        "    if os.path.exists(\"library/model_util.py\"):\n",
        "      !sed -i \"s@cpu@cuda@\" library/model_util.py\n",
        "\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    from pathlib import Path\n",
        "\n",
        "    candidates = [\n",
        "      Path(\"library/train_util.py\"),\n",
        "      Path(\"train_util.py\"),\n",
        "      Path(\"library/utils.py\"),\n",
        "    ]\n",
        "\n",
        "    target = next((p for p in candidates if p.exists()), None)\n",
        "\n",
        "    if target is None:\n",
        "      print(\"No encontr√© train_util.py (ni similares). Skip patch LOAD_TRUNCATED_IMAGES.\")\n",
        "    else:\n",
        "      s = target.read_text()\n",
        "      needle = \"from PIL import Image\"\n",
        "      insert = \"from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True\"\n",
        "\n",
        "      if \"ImageFile.LOAD_TRUNCATED_IMAGES\" not in s and needle in s:\n",
        "        s = s.replace(needle, insert, 1)\n",
        "        target.write_text(s)\n",
        "        print(f\"Patched {target} OK\")\n",
        "      else:\n",
        "        print(f\"Patch ya aplicado o no encontr√© '{needle}' en {target}.\")\n",
        "\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    if os.path.exists(\"library/train_util.py\"):\n",
        "      !sed -i 's/{:06d}/{:02d}/g' library/train_util.py\n",
        "    if os.path.exists(\"train_network.py\"):\n",
        "      !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "    if not os.path.exists(accelerate_config_file):\n",
        "      write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "    os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, keep_tokens_weight, weighted_captions, adjust_tags\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\nüíø Revisando archivos...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"üí• Error: Por favor elije un nombre de proyecto v√°lido.\")\n",
        "    return\n",
        "\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"üí• Error: ¬°Tu configuraci√≥n de datos propia es inv√°lida o tiene errores! Por favor revisa el ejemplo original.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"üí• Error: La carpeta {folder.replace('/content/drive/', '')} no existe.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"üí• Error: La carpeta {folder.replace('/content/drive/', '')} est√° vac√≠a.\")\n",
        "      return\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((\".txt\", \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"üí• Error: Archivo inv√°lido encontrado: \\\"{f}\\\". Abortando.\")\n",
        "      return\n",
        "\n",
        "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"üí• Error: Archivo de continuar_lora inv√°lido. Ejemplo: /content/drive/MyDrive/Loras/ejemplo.safetensors\")\n",
        "    return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"üìÅ\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularizaci√≥n)\" if folder in reg else \"\"))\n",
        "  print(f\"üìà Se encontraron {img} im√°genes con {rep} repeticiones, equivalente a {img*rep} pasos.\")\n",
        "  print(f\"üìâ Divide {pre_steps_per_epoch} pasos en {train_batch_size} batch size para obtener {steps_per_epoch} pasos por epoch.\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"üîÆ Habr√° {max_train_epochs} epochs, para un total de alrededor de {total_steps} pasos totales.\")\n",
        "  else:\n",
        "    print(f\"üîÆ Habr√° {total_steps} pasos, divididos en {estimated_epochs} epochs y un poco m√°s.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"üí• Error: Tus pasos totales on muy altos. Probablemente cometiste un error. Abortando...\")\n",
        "    return\n",
        "\n",
        "  if adjust_tags:\n",
        "    print(f\"\\nüìé Weighted tags: {'ON' if weighted_captions else 'OFF'}\")\n",
        "    if weighted_captions:\n",
        "      print(f\"üìé Will use {keep_tokens_weight} weight on {keep_tokens} activation tag(s)\")\n",
        "    print(\"üìé Adjusting tags...\")\n",
        "    adjust_weighted_tags(folders, keep_tokens, keep_tokens_weight, weighted_captions)\n",
        "\n",
        "  return True\n",
        "\n",
        "def adjust_weighted_tags(folders, keep_tokens: int, keep_tokens_weight: float, weighted_captions: bool):\n",
        "  weighted_tag = re.compile(r\"\\((.+?):[.\\d]+\\)(,|$)\")\n",
        "  for folder in folders:\n",
        "    for txt in [f for f in os.listdir(folder) if f.lower().endswith(\".txt\")]:\n",
        "      with open(os.path.join(folder, txt), 'r') as f:\n",
        "        content = f.read()\n",
        "      # reset previous changes\n",
        "      content = content.replace('\\\\', '')\n",
        "      content = weighted_tag.sub(r'\\1\\2', content)\n",
        "      if weighted_captions:\n",
        "        # re-apply changes\n",
        "        content = content.replace(r'(', r'\\(').replace(r')', r'\\)').replace(r':', r'\\:')\n",
        "        if keep_tokens_weight > 1:\n",
        "          tags = [s.strip() for s in content.split(\",\")]\n",
        "          for i in range(min(keep_tokens, len(tags))):\n",
        "            tags[i] = f'({tags[i]}:{keep_tokens_weight})'\n",
        "          content = \", \".join(tags)\n",
        "      with open(os.path.join(folder, txt), 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n‚≠ï Usando configuraci√≥n propia {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"additional_network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"noise_offset\": None,\n",
        "        \"clip_skip\": 1,\n",
        "        \"min_snr_gamma\": min_snr_gamma_value,\n",
        "        \"weighted_captions\": weighted_captions,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": XFORMERS,\n",
        "        \"lowram\": COLAB,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"output_dir\": output_folder,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"output_name\": project_name,\n",
        "        \"log_prefix\": project_name,\n",
        "      },\n",
        "      \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"v2\": custom_model_is_based_on_sd2,\n",
        "        \"v_parameterization\": True if custom_model_is_based_on_sd2 else None,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "      },\n",
        "      \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "      },\n",
        "      \"dataset_arguments\": {\n",
        "        \"cache_latents\": False,\n",
        "      },\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\nüìÑ Configuraci√≥n guardada en {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"‚≠ï Usando configuraci√≥n de datos propia {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "        \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"üìÑ Configuraci√≥n de datos guardada en {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url.strip()\n",
        "\n",
        "  if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "  else:\n",
        "    model_file = \"/content/downloaded_model.safetensors\"\n",
        "    if os.path.exists(model_file):\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)\", model_url):\n",
        "    real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"Modelo renombrado a {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"üìÇ Conectando a Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\nüè≠ Instalando...\\n\")\n",
        "    t0 = time()\n",
        "    install_dependencies()\n",
        "    t1 = time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\n‚úÖ Instalaci√≥n completada en {int(t1-t0)} segundos.\")\n",
        "  else:\n",
        "    print(\"\\n‚úÖ Ya se ha realizado la instalaci√≥n.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nüîÑ Descargando modelo...\")\n",
        "    if not download_model():\n",
        "      print(\"\\nüí• Error: El modelo que elegiste es inv√°lido o est√° corrupto, o no se pudo encontrar. Recomiendo usar un enlace de huggingface o civitai.\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\nüîÑ El modelo ya ha sido descargado.\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n‚≠ê Iniciando entrenador...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}